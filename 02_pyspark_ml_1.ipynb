{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "<b>Dataset location: </b>https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset <br />\n",
    "Number of riders using a bikeshare service on a given day. We will predict the number of riders given information about the type of day and weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Effects of Dimensionality Reduction when making predictions') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "rawData = spark.read\\\n",
    "            .format('csv')\\\n",
    "            .option('header', 'true')\\\n",
    "            .load('data/day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select required columns from data\n",
    "* The <i>instant</i> and <i>dteday</i> columns are dropped as they are unique, thus not useful for predictions\n",
    "* The <i>casual</i> and <i>registered</i> fields will sum up to the cnt field which we wish to predict, so we remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataset = rawData.select(col('season').cast('float'),\n",
    "                         col('yr').cast('float'),\n",
    "                         col('mnth').cast('float'),\n",
    "                         col('holiday').cast('float'),\n",
    "                         col('weekday').cast('float'),\n",
    "                         col('workingday').cast('float'),\n",
    "                         col('weathersit').cast('float'),\n",
    "                         col('temp').cast('float'),\n",
    "                         col('atemp').cast('float'),\n",
    "                         col('hum').cast('float'),\n",
    "                         col('windspeed').cast('float'),\n",
    "                         col('cnt').cast('float')\n",
    "                        )\n",
    "\n",
    "dataset.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check correlation between fields\n",
    "* <i>temp</i> and <i>atemp</i> are almost perfectly correlated\n",
    "* <i>month</i> and <i>season</i> are also strongly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corrmat = dataset.toPandas().corr()\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.set(font_scale=1.0)\n",
    "sns.heatmap(corrmat, vmax=.8, square=True, annot=True, fmt='.2f', cmap = \"winter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select features\n",
    "All fields except the <i>cnt</i> field make up our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = dataset.columns.copy()\n",
    "featureCols.remove('cnt')\n",
    "\n",
    "featureCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The total number of features\n",
    "We will reduce the dimensions later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(featureCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the feature vector using an assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=featureCols,\n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorDF = assembler.transform(dataset)\n",
    "vectorDF.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = vectorDF.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple Linear Regression model\n",
    "We will not aim to get the best parameters here as our aim is to compare the model when used with regular features and then features with transformed and reduced dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=100,\n",
    "                      regParam=1.0,\n",
    "                      elasticNetParam=0.8,\n",
    "                      labelCol='cnt', \n",
    "                      featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate R-square and RMSE on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training R^2 score = ', model.summary.r2)\n",
    "print('Training RMSE = ', model.summary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-square score on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='cnt', \n",
    "    predictionCol='prediction', \n",
    "    metricName='r2')\n",
    "\n",
    "rsquare = evaluator.evaluate(predictions)\n",
    "print(\"Test R^2 score = %g\" % rsquare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='cnt', \n",
    "    predictionCol='prediction', \n",
    "    metricName='rmse')\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('Test RMSE = ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert predictions dataframe to Pandas dataframe\n",
    "This will make it easier for us to create a series which we will use to plot the actual and predicted values of <i>cnt</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsPandas = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare actual and predicted values of cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(predictionsPandas['cnt'], label='Actual')\n",
    "plt.plot(predictionsPandas['prediction'], label='Predicted')\n",
    "\n",
    "plt.ylabel('Ride Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis\n",
    "Performs an orthogonal transformation to convert a set of possibly correlated variables into a set of values of linearly uncorrelated variables called <b>principal components</b>\n",
    "* the pcaTransformer will extract the principal components from the features\n",
    "* the number of components is set by the value of <b>k</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=8, \n",
    "          inputCol='features', \n",
    "          outputCol='pcaFeatures'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaTransformer = pca.fit(vectorDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the principal components in the transformed space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaFeatureData = pcaTransformer.transform(vectorDF).\\\n",
    "                select('pcaFeatures')\n",
    "\n",
    "pcaFeatureData.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The principal components are stored as a DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaFeatureData.toPandas()['pcaFeatures'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the  Explained Variance \n",
    "This shows the eigen values of each principal component in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaTransformer.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scree plot\n",
    "Visualise the explained variance. This can be used to eliminate dimensions with low eigen values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(pcaTransformer.explainedVariance)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Explain Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a dataset to use with the Linear Regression model\n",
    "* We need to prepare a dataframe containing the principal components and the correct value of cnt\n",
    "* We will retrieve the <i>pcaFeatures</i> column from pcaFeatureData and the <i>cnt</i> column from vectorDF\n",
    "* In order to join these two dataframes, we need to create a column on which to join - for that we add a <i>row_index</i> field to each dataframe which we will use to perform the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "pcaFeatureData = pcaFeatureData.withColumn('row_index', monotonically_increasing_id())\n",
    "vectorDF = vectorDF.withColumn('row_index', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the tables using the row_index field\n",
    "We only extract the <i>cnt</i> and <i>pcaFeatures</i> fields which we require from the joined table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = pcaFeatureData.join(vectorDF, on=['row_index']).\\\n",
    "                sort('row_index').\\\n",
    "                select('cnt', 'pcaFeatures') \n",
    "        \n",
    "transformedData.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the training and test datasets from our new transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pcaTrainingData, pcaTestData) = transformedData.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the LinearRegression model\n",
    "This has the exact same parameters as our previous model for a meaningful comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcalr = LinearRegression(maxIter=100,\n",
    "                      regParam=1.0,\n",
    "                      elasticNetParam=0.8,\n",
    "                      labelCol='cnt', \n",
    "                      featuresCol='pcaFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaModel = pcalr.fit(pcaTrainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSE and R-square values on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training R^2 score = ', pcaModel.summary.r2)\n",
    "print('Training RMSE = ', pcaModel.summary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform predictions using the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaPredictions = pcaModel.transform(pcaTestData)\n",
    "pcaPredictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate R-square on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='cnt', \n",
    "    predictionCol='prediction', \n",
    "    metricName='r2')\n",
    "\n",
    "rsquare = evaluator.evaluate(pcaPredictions)\n",
    "print(\"Test R^2 score = %g\" % rsquare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='cnt', \n",
    "    predictionCol='prediction', \n",
    "    metricName='rmse')\n",
    "\n",
    "rmse = evaluator.evaluate(pcaPredictions)\n",
    "print('Test RMSE = ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert predictions dataframe to a pandas dataframe\n",
    "This will allow us to plot a graph of the predicted values against the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaPredictionsPandas = pcaPredictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(pcaPredictionsPandas['cnt'], label='Actual')\n",
    "plt.plot(pcaPredictionsPandas['prediction'], label='Predicted')\n",
    "\n",
    "plt.ylabel('Ride Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
